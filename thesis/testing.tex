\chapter{Testing}
This chapter describes how testing is set up and managed a cross the applications and how \glsi{ci} is used.
The test setup is mainly a \gls{poc}, where we evaluate how the different frameworks supports testing and what implications are encountered during the process of setting it up and running the tests.
This knowledge is used to reflect on the framework choices made in the pre-studies, chapter \ref{ch:preStudy}.

The applications have limited business logic which results in code that benefits very little from testing and because of this, it is decided to spend as much time evaluating the framework choices instead of writing test for simple functionality.

\section{Front-end}
\Glsi{nativescript} have built in unit testing functionality to run tests on hardware or emulated devices \citep{testing:nativescript}.
When setting this up, there were, for now, an unfixable complication caused by the front-end application build configuration.
The project utilizes \glsi{webpack} to bundle and manipulate the source code at compile time and this is not compatible with the unit testing flow of \glsi{nativescript}.

As of this, we decided to test the limited business logic of the application with a stand-alone testing framework running on \glsi{node}.
As \glsi{node} runs on the \glsi{v8} run-time, same as \textit{Android}, it still provides valid results though not running the tests on actual devices.

The business logic is only unit tested as integration tests are not possible without the \glsi{nativescript} testing framework.

\section{Back-end}
The back-end application is tested on two levels.
Unit tests, that are run on isolated functions where dependencies are mocked away and integration tests with nothing mocked away.

There are several web resources on unit testing and code coverage calculation for \glsi{node}.
But when using \gls{typescript} the code coverage calculations should be made on the original source code, not the transpiled JavaScript, and this is not that straight forward.
But with research and \textit{trial and error} we came up with a solution that transpiles the \gls{typescript} test files on the fly and calculate code coverage as well.
This works, but there still is an issue when calculating the complete coverage of the application.
Due to the build setup, the tool used to run tests cannot read all files and there fore the total coverage result is misleading.

\subsubsection{Unit tests}
Unit tests tests isolated parts of the application.
They test functions at the different layers, where the below layer will be mocked away to make the tests isolated.
These are the simplest form of tests but are still a valuable tool to create a durable and well-tested application.

\subsubsection{Integration tests}
Integration tests work differently than the unit tests.
They test the application as a whole.
This means that these tests only go well, if no errors occur in any layer of the application.
A top-down approach is used \citep[pp. 662-669]{testing:integration}.
The integration tests start up the \glsi{node} server, and then starts querying against its endpoints.
When tests are run, a test database is used, which is seeded with known data every time the server starts in testing mode.

\subsubsection{Staging}
\label{sec:staging}
A staging environment is used to make sure that tests are run independently of the production environment.
Two environment variables are important in this context.
\verb+NODE_ENV+ and \verb+ROM_DB_TEST+ are set on the \glsi{travis} server to make sure that the right things happen.

When \verb+NODE_ENV+ is set to \verb+test+, the database will be seeded upon server start.
That should only ever happen in a testing or developer environment and never in a production environment.

To avoid reseeds of the production database, two different environment variables are used.
When \verb+NODE_ENV+ is set to \verb+test+, the \verb+ROM_DB_TEST+ URL is used.
When \verb+NODE_ENV+ is set to \verb+prod+, the \verb+ROM_DB_PROD+ URL is used.
When running tests locally, \verb+NODE_ENV+ is automatically set to \verb+test+.
See chapter \ref{doc:ch:environmentVariables} in the documentation for details on the available environment variables.

A developer can run his code locally against the production database if needed, but will never accidentally reseed the database.

\section{Continuous Integration}
A \glsi{travis} server is set up to run builds, tests and more for both the front and back-end applications.

When commits are made and pushed to any one of the git repositories \glsi{travis} runs different tasks and verify the integrity of the commit.

What tasks to run is configured with a \verb+.travis.yml+ file in the root of the repository and includes:

\begin{enumerate}
\item \textbf{Linting}: Code style is tested to keep code quality high and consistent
\item \textbf{Build}: The source code is compiled
\item \textbf{Tests}: Unit tests, and integration tests on back-end, are run to test code functionality
\item \textbf{Code coverage}: Helps keeping track of what parts of the code is tested
\end{enumerate}

Should any of these tasks fail, the user committing the code, will receive an email notification.
This helps both the developer who made the change, but also the developer that has to review the changes.

When a pull request is reviewed, it contains information about the test results and what the code coverage is.
If code coverage decreases in the pull request, compared to the master branch, then the developer probably did not test the code well enough.
The lint check helps the reviewer to focus on the functional parts instead of syntactic and code style things.

\input{thesis/tests/accepttest}

\section{Conclusion}
On the back-end, the test setup became complicated as we used \gls{typescript}.
The final setup is not that complicated, but finding the right libraries and tools to get the job done proved more cumbersome than expected.
There is still an issue with the total coverage calculation that should be solved.

On the front-end, the test setup was, at first glance, easy to set up, but in the process we found out that this was actually not the case.
Due to this we had to make a choice if we wanted no tests at all or some representative tests to test core logic.
We decided on the latter and replicated the back-end testing setup.
This is working all right, but testing the source code on actual devices are preferred, especially when the JavaScript run-times differ between \textit{iOS} and \textit{Android}.

Enabling the \glsi{ci} server was one of the easiest task in the setup.
The \glsi{travis} service integrates with \glsi{github} and was literally activated with just a few clicks on their website.
The special needs for installing \textit{Android} \glspl{sdk}, to build the front-end, were setup by studying \glsi{nativescript} sample repositories \citep{testing:nativescript:sample}.

